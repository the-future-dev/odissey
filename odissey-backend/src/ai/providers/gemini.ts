import {
  AIProvider,
  AIModality,
  TextToTextRequest,
  TextToTextResponse,
  AIProviderError,
  UnsupportedModalityError
} from '../interfaces';

export interface GeminiConfig {
  apiKey: string;
  baseUrl?: string;
  model?: string;
}

export class GeminiProvider implements AIProvider {
  readonly name = 'gemini';
  readonly supportedModalities = [AIModality.TextToText];

  private apiKey: string;
  private baseUrl: string;
  private model: string;

  constructor(config: GeminiConfig) {
    this.apiKey = config.apiKey;
    this.baseUrl = config.baseUrl || 'https://generativelanguage.googleapis.com/v1beta';
    this.model = config.model || 'gemini-1.5-flash';
  }

  async generateText(request: TextToTextRequest): Promise<TextToTextResponse> {
    if (!this.supportedModalities.includes(AIModality.TextToText)) {
      throw new UnsupportedModalityError(AIModality.TextToText, this.name);
    }

    try {
      // Convert messages to Gemini format
      const contents = this.formatMessages(request.messages);
      
      const payload = {
        contents,
        generationConfig: {
          temperature: request.temperature || 0.7,
          maxOutputTokens: request.maxTokens || 500,
          ...(request.stopSequences && { stopSequences: request.stopSequences })
        }
      };

      console.log(`Calling Gemini API with model: ${this.model}`);
      
      const response = await fetch(
        `${this.baseUrl}/models/${this.model}:generateContent?key=${this.apiKey}`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(payload),
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        console.error('Gemini API Error Response:', errorText);
        throw new AIProviderError(
          `Gemini API error: ${errorText}`,
          this.name,
          AIModality.TextToText,
          response.status
        );
      }

      const data = await response.json() as any;
      console.log('Gemini API Response structure:', JSON.stringify({
        candidatesCount: data.candidates?.length || 0,
        hasContent: !!data.candidates?.[0]?.content,
        finishReason: data.candidates?.[0]?.finishReason
      }));
      
      if (!data.candidates || data.candidates.length === 0) {
        throw new AIProviderError(
          'No response generated by Gemini',
          this.name,
          AIModality.TextToText
        );
      }

      const candidate = data.candidates[0];
      const content = candidate.content?.parts?.[0]?.text || '';

      if (!content.trim()) {
        throw new AIProviderError(
          'Empty response generated by Gemini',
          this.name,
          AIModality.TextToText
        );
      }

      return {
        content: content.trim(),
        model: this.model,
        finishReason: candidate.finishReason || 'stop'
      };

    } catch (error) {
      if (error instanceof AIProviderError) {
        throw error;
      }
      
      const errorMessage = error instanceof Error ? error.message : String(error);
      console.error('Gemini Provider Error:', errorMessage, error);
      throw new AIProviderError(
        `Failed to generate text: ${errorMessage}`,
        this.name,
        AIModality.TextToText
      );
    }
  }

  async generateTextStream(request: TextToTextRequest): Promise<TextToTextResponse> {
    if (!this.supportedModalities.includes(AIModality.TextToText)) {
      throw new UnsupportedModalityError(AIModality.TextToText, this.name);
    }

    try {
      // Convert messages to Gemini format
      const contents = this.formatMessages(request.messages);
      
      const payload = {
        contents,
        generationConfig: {
          temperature: request.temperature || 0.7,
          maxOutputTokens: request.maxTokens || 500,
          ...(request.stopSequences && { stopSequences: request.stopSequences })
        }
      };

      console.log(`Calling Gemini Streaming API with model: ${this.model}`);
      
      // Add alt=sse parameter for proper SSE format
      const response = await fetch(
        `${this.baseUrl}/models/${this.model}:streamGenerateContent?alt=sse&key=${this.apiKey}`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(payload),
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        console.error('Gemini streaming failed:', response.status, errorText);
        console.warn('Falling back to regular generation');
        return await this.generateText(request);
      }

      if (!response.body) {
        throw new Error('No response body for Gemini streaming');
      }

      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullContent = '';
      let buffer = '';

      try {
        while (true) {
          const { done, value } = await reader.read();
          
          if (done) break;
          
          const chunk = decoder.decode(value, { stream: true });
          buffer += chunk;
          
          // Process complete lines ending with \n\n (SSE format)
          const lines = buffer.split('\n');
          // Keep the last incomplete line in buffer
          buffer = lines.pop() || '';
          
          for (const line of lines) {
            const trimmedLine = line.trim();
            if (!trimmedLine) continue;
            
            // Parse SSE data lines
            if (trimmedLine.startsWith('data: ')) {
              const jsonData = trimmedLine.slice(6);
              
              if (jsonData && jsonData !== '[DONE]') {
                try {
                  const data = JSON.parse(jsonData);
                  
                  if (data.candidates && data.candidates[0]?.content?.parts?.[0]?.text) {
                    const textChunk = data.candidates[0].content.parts[0].text;
                    
                    // Clean up potential streaming artifacts
                    const cleanedChunk = textChunk.replace(/replacesWith/g, '').trim();
                    
                    if (cleanedChunk) {
                      fullContent += cleanedChunk;
                      
                      if (request.onChunk) {
                        request.onChunk(cleanedChunk);
                      }
                    }
                  }
                } catch (parseError) {
                  console.warn('Failed to parse streaming JSON:', parseError, 'Data:', jsonData);
                }
              }
            }
          }
        }
      } finally {
        reader.releaseLock();
      }

      console.log('Gemini streaming completed. Full content length:', fullContent.length);
      
      if (!fullContent.trim()) {
        console.error('No content received from Gemini streaming. Buffer had content but extraction failed.');
        throw new AIProviderError(
          'No content received from Gemini streaming',
          this.name,
          AIModality.TextToText
        );
      }

      return {
        content: fullContent.trim(),
        model: this.model,
        finishReason: 'stop'
      };

    } catch (error) {
      if (error instanceof AIProviderError) {
        console.error('Gemini streaming failed with AIProviderError:', error.message);
        // Fall back to regular generation
        console.log('Falling back to regular Gemini generation');
        try {
          const fallbackRequest = { ...request };
          delete fallbackRequest.onChunk; // Remove streaming callback
          return await this.generateText(fallbackRequest);
        } catch (fallbackError) {
          console.error('Fallback to regular generation also failed:', fallbackError);
          throw error; // Throw original streaming error
        }
      }
      
      console.error('Gemini streaming failed with unexpected error:', error);
      // Fall back to regular generation
      console.log('Falling back to regular Gemini generation');
      try {
        const fallbackRequest = { ...request };
        delete fallbackRequest.onChunk; // Remove streaming callback
        return await this.generateText(fallbackRequest);
      } catch (fallbackError) {
        console.error('Fallback to regular generation also failed:', fallbackError);
        throw new AIProviderError(
          `Streaming failed and fallback failed: ${error instanceof Error ? error.message : String(error)}`,
          this.name,
          AIModality.TextToText
        );
      }
    }
  }

  private formatMessages(messages: Array<{ role: string; content: string }>) {
    const contents = [];
    
    for (const message of messages) {
      if (message.role === 'system') {
        // Gemini doesn't have a system role, so we'll add it as part of the first user message
        continue;
      }
      
      contents.push({
        role: message.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: message.content }]
      });
    }
    
    // Add system message to first user message if exists
    const systemMessage = messages.find(m => m.role === 'system');
    if (systemMessage && contents.length > 0 && contents[0].role === 'user') {
      contents[0].parts[0].text = `${systemMessage.content}\n\n${contents[0].parts[0].text}`;
    }
    
    return contents;
  }
} 